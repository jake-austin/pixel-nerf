# Main multiview supported config
model {
    # MLP architecture
    # Adapted for multiview
    # Possibly too big
    mlp_oracle {
        type = memes
        d_hidden = 512
        output_bins = 8
    }
    mlp_fine {
        type = resnet
        n_blocks = 5
        d_hidden = 512
        combine_layer = 3
        combine_type = average
    }
    encoder {
        # Skip first pooling layer to avoid reducing size too much
        use_first_pool=False
    }
    num_oracle_training_rays = 256
    type = oracle
}
data {
    format = dvr
}
renderer {
    n_coarse = 128
    n_fine = 8
    oracle_train_samples = 128
    # Try using expected depth sample
    n_fine_depth = 0
    # Noise to add to depth sample
    depth_std = 0.01
    # Decay schedule, not used
    sched = []
    # White background color (false : black)
    white_bkgd = True
}
loss {
    # RGB losses coarse/fine
    rgb {
        use_l1 = False
    }
    rgb_fine {
        use_l1 = False
    }
    # Alpha regularization (disabled in final version)
    alpha {
        # lambda_alpha = 0.0001
        lambda_alpha = 0.0
        clamp_alpha = 100
        init_epoch = 5
    }
    # Coarse/fine weighting (nerf = equal)
    lambda_oracle = 1.0  # loss = lambda_coarse * loss_coarse + loss_fine
    lambda_fine = 1.0  # loss = lambda_coarse * loss_coarse + loss_fine
}
train {
    # Training 
    print_interval = 2
    save_interval = 50
    vis_interval = 100
    eval_interval = 50

    # Accumulating gradients. Not really recommended.
    # 1 = disable
    accu_grad = 1

    # Number of times to repeat dataset per 'epoch'
    # Useful if dataset is extremely small, like DTU
    num_epoch_repeats = 1
}
